# @package _global_
defaults:
  - /data: sft/llama
  - /model/backbone: sequence_interleave
  - /model/backbone/layer@model.backbone.seq_cell: rat
  - /model/backbone/layer@model.backbone.seq_cell1: attention
  - /model/backbone/layer@model.backbone.hidden_cell: ffn
  - /model/head: lm
  - /model/embedding: lm
  - /model/embedding@model.embedding.pe: rope_interrope
  - /task: lm
  - /optim/lr_scheduler: cosine
  - /optim/optimizer: adamw
  - _self_

data:
  _name_: narrativeqa
  global_batch_size: 128
  seq_len: 4096
  train:
    limit_tokens: -1

optim:
  optimizer:
    lr: 1.0e-5
    weight_decay: 0.01
    betas: [0.9,0.95]
  lr_scheduler:
    warmup_iter: 0.05 # keep the same with the pre-training
    eta_min: 1.0e-6

wandb:
  job_type: sft_${..data._name_}


model:
  backbone:
    num_layers: 24
    d_model: 2048
    dropout: 0.0
    bias: false
    ln: rmsnorm
    interleave_step: 12
    init:
      _name_: fixed
      initializer_range: 0.02
    seq_cell:
      d_head: 128
      chunk_size: 16
    seq_cell1:
      d_head: 128
task:
  ignore_index: -100

trainer:
  global_batch_size: ${data.global_batch_size}
  gradient_clipping: 1.0
  max_epoch: 1
  load_checkpoint: true
  save_checkpoint: true
  save_interval: 1.0
  log_interval: 20
  eval_when_log: true
  dtype: bfloat16
  pretrained_path: /home/anonymous/fake_path/sequence_model/pretrain_1B_100BT_fineweb/attention_ratl${..model.backbone.seq_cell.chunk_size}_l${..model.backbone.interleave_step}.pth